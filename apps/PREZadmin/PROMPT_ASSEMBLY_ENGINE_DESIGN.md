# Prompt Assembly Engine Design Document

## Executive Summary

This system's primary function is as a **prompt assembly engine** to craft special-purpose prompts for external AI systems (Claude, ChatGPT, Gemini, etc.). It will use a form of ontology-like metadata to organize data into logical, 300 token or smaller sized chunks.

**Core Function**: Produce a specific set of very tailored prompts, transforming an engineering-like intent into a contextually rich prompt.

## Feature Limitations

**Target User Base**: This is an expert-only tool designed for 2-5 advanced prompt engineers (only 1 user for the first year). Such users would have deep understanding of prompt engineering, ontological relationships, and AI system limitations.

**Consumer Application Boundary**: This system is NOT designed for end-users. A separate consumer application (out of scope for this design) will eventually consume the prompts generated by this system, presenting them to untrained users at a System 1 fluency level. The relationship between this expert tool and the future consumer app is limited to ensuring adequate data structure and prompt formats for downstream consumption.

**Interim Product**: Final product will be AI prompts with comprehensive context. Interim products will differ, as the prompt will have to evolve iteratively, starting as plain answers, and gradually evolving into solid prompts that perform well inside mainstream AI engines.

**Other uses**: This app may also have ancillary uses, more often than not just experiments to learn more about how a RAG use cases can evolve. It is, however, **not designed as a traditional RAG Q&A application**.

## Problem Definition - Ontology vs Simple Text Chunking

This system is designed to increase chances of a more deterministic result, while simultaneously improving the quality of content. The following explains how.

### The Narrative Advantage

Humans and LLMs get along well, because humans like a narrative and an LLM can chunk any narrative up into digestable inputs.

Where this advantage can become a hairball, is when attempting to design engineered systems around logic structures and abstractions, rather than a narrative. You wouldn't engineer an industrial plant based on stories, you would use abstractions, logic trees, and specifications. Each pipe, piece of equipment or even bolt type, might have it's own defining document.

The engineering team writing the abstractions, logic trees and specifications would greatly prefer to write the chunks as very distinct and discrete documents, each outlining it's bounds, inclusions, exclusions. For this, an ontological approach is usually easier on the engineering team, although not necessarily for the LLM, which then has the extra burden of traversing the ontology, just to aggregate the relevant output, no matter what that output might be - a stress analysis test description, a plan, or a material specification.

### The Ontological Advantage

The counter-example to the narrative advantage can be illustrated by comparing an LLM to a properly normalized bank database with millions of records. The bank database would tell me that i have exactly $1368.21 in that account. At best, a modestly powered LLM with millions of unrelated chunks including bank records in narrative form might do well to guess a range. "You probably have over $1000 in your account, not sure."

So for engineering design, or prompts leading to proper engineerng design, it can be helpful for the app user to be able to use filters and metadata to help the LLM reduce scope and target it's focus, thus increasing probability of a more deterministic engineering match.

Thus, there is a natural tension between this RAG's design:
  - RAG design would be easier with just a large stack of chunks to feed the RAG
  - Authoring engineering chunks is easier with chunks that are only designed to work with each other within more structured selections, almost like an ontology or even structured database. 
  
Neither of these extremes must win
  - Structures are never pure enough to not need the LLM to add it's own special selection capabilities to ferret out exceptions and edge cases, - - Handing the giant stack of chunks to select from, is probably too extreme, as well, being too probablistic rather than deterministic.

### Abstraction Leakage

Another challenge specific to this system is that of abstraction leakage.

The prompts this app is designe for, by necessity, include social engineering questions. Abstractions, in this area, are notoriously leaky. One person's assistance is another person's abuse. This creates a nightmare scenario for LLMs, which must profer matches based on assumptions about meaning and intent that may not be represented properly by the chunks provided in this RAG.

Once again, ontological metadata may offer benefits in this area. It is not knowable, without experimentation, when this will, or will not be adequate or usable. But even if only effective in some use cases, it may offer a big lift.

### The Human Challenge (Expert User Context)
- Expert prompt engineers are "notoriously inept at knowing what we want until offered an unappealing choice". This is especially true of combining engineering chunks into workable prompt structure.
- Expert-level prompt design for engineering shouldn't need humans to create perfect ontological structures to benefit from the filtering that allows LLMs to reduce the probablistic nature of their selection errors.
- Expert users need flexible context assembly while allowing SQL-level query complexity, but can handle sophisticated interfaces that would overwhelm general users.
- Expert users value powerful, precise tools over simplified interfaces - they prefer functionality depth over ease-of-use for novices.

### The Prompt Quality Challenge
- Generic prompts can produce probablistic results when more deterministic results are within easy reach.
- High-quality prompts require comprehensive context
- Context assembly should be intelligent, not just keyword-based
- Different prompt purposes require different context strategies

## System Architecture Vision

### Current State: Two Separate Pipelines
```
Semantic Search → Vector Similarity → LLM Response
Ontological Search → Metadata Filter → Raw Results (No LLM)
```

### Target State: Multi-Database Two-Pass Prompt Assembly Engine

hey pete edit this because it is expecting tech and you are specifying performance or something

#### Pass 1 - Recursive Content Aggregation

In this mode, the expert uses ontological and other data, in combination with standard querying techniques, to add content to the context.
  - User can run as many requests as needed to add any document(s) via metadata.
  - User can extend content selection via LLM assisted queries.
  - User can delete any individual content added by any queries.
  - Acceptance of content aggregated triggers migration to Pass 2
```

### Pass 2 - Prompt Generation

In this mode, the expert uses 

Pass 1: Firestore metadata queries → Recursive filtering → Document ID list
Pass 2A (Local): Content retrieval → Local LLM (128k tokens) → Prompt variations
Pass 2B (External): Content retrieval → External LLM API → Prompt variations
```

### Revised Data Architecture
```
Document Storage:
- Local Files: ../RAG/[filename].md (source of truth)
- ChromaDB: Vector embeddings + content (semantic search capability)
- Firestore: Structured metadata + full content (Pass 1 queries + backup)
- Total: 4-6 write operations per document (including emulator/remote Firestore)
```

### Recursive Pass 1 Architecture (Firestore-Based)
```
Initial Query → Firestore metadata filter → Document Set A
↓ User: "Add child_of=pedagogy" 
→ Enhanced Firestore query → Document Set B (A + new documents)
↓ User: "Remove these 3 irrelevant documents"
→ Manual curation → Document Set C (B - selected documents)
↓ User: "Undo last removal"
→ Operation stack rollback → Back to Document Set B
↓ User: "Add semantic search enhancement"
→ ChromaDB similarity search on Set B → Document Set D (B + semantically related)
```

## Core Design Principles

### 1. Expert-Level Precise Interaction
- **User Experience**: Sophisticated intent specification with ontological control ("I want to create a prompt about teaching methodologies, expand via child_of relationships, exclude theoretical-only approaches")
- **System Behavior**: Visible, controllable ontological traversal with iterative refinement capabilities
- **User Output**: Multiple prompt variations with full context transparency and editing capability

### 2. Relevance Over Speed
- **Performance Philosophy**: Better to take time and get comprehensive context than fast, shallow results
- **Caching Strategy**: Cache common ontological traversals and prompt patterns
- **Iteration Support**: Allow refinement of generated prompts

### 3. Recursive Ontological Intelligence
- **Relationship Awareness**: Traverse `is_a`, `child_of`, `has_a` relationships with iterative refinement
- **Context Strategies**: Support different assembly approaches (comprehensive, focused, comparative) with undo/redo capabilities
- **Dynamic Discovery**: Let data relationships guide context inclusion through expert-controlled recursive filtering
- **Expert Control**: Full visibility into filtering decisions with granular add/remove capabilities

## Use Cases and User Stories

### Primary Use Case: Intent-to-Prompt Transformation

**Scenario**: User needs a prompt about effective teaching methods for adult learners

**Current Process** (Manual):
1. User searches for "teaching methods"
2. User searches for "adult learning" 
3. User manually assembles context from various documents
4. User crafts prompt with limited perspective

**Target Process** (Automated):
1. User: "Create a prompt about effective teaching methods for adult learners"
2. System: 
   - Finds documents `is_a: teaching-method`
   - Traverses to `child_of: adult-education` 
   - Includes `has_a: research-evidence` support
   - Discovers contrasting methodological approaches
3. Output: Comprehensive prompt ready for external AI

### Secondary Use Cases

**Prompt Iteration**: "Make this prompt more comprehensive" or "Focus this prompt on practical applications"

**Context Exploration**: "Show me what related concepts exist before I decide on my prompt focus"

**Template Generation**: "Create a reusable prompt template for this type of query"

## Ontological Traversal Strategies

### Strategy 1: Comprehensive Context Assembly
- Follow all relationship paths from core concepts
- Include supporting evidence and contrasting viewpoints
- Generate prompts with maximum context richness

### Strategy 2: Focused Context Assembly  
- Identify core concepts only
- Limit relationship traversal depth
- Generate concise, targeted prompts

### Strategy 3: Comparative Context Assembly
- Seek out contrasting approaches or viewpoints
- Include `child_of` relationships that show different schools of thought
- Generate prompts that ask external AI to compare/contrast

### Strategy 4: Evidence-Based Context Assembly
- Prioritize documents with `has_a: research-basis` or `has_a: evidence`
- Include methodology and source credibility information
- Generate prompts optimized for factual accuracy

## User Interaction Models

### Model 1: Natural Language Intent (Primary)
**Input**: "I need a prompt about database design principles for e-commerce applications"
**Process**: System interprets intent, traverses ontology, assembles context
**Output**: Complete prompt ready for external AI

### Model 2: Recursive Context Refinement (Primary for Experts)
**Input**: Initial ontological query (e.g., "is_a: teaching-method") informed by metadata visualization scan
**Process**: 
0. Expert scans metadata visualization to identify relationship patterns and plan filtering strategy
1. System returns initial document set based on planned query
2. User iteratively expands ("add child_of=pedagogy") guided by visualization insights
3. User removes irrelevant documents individually
4. System maintains undo/redo stack of operations
5. User fine-tunes document set through multiple cycles
6. Expert can return to metadata visualization to identify additional expansion opportunities
**Output**: Precisely curated context set ready for Pass 2 prompt generation

### Model 3: Template-Based Assembly (Tertiary)
**Input**: User selects from common prompt patterns
**Process**: System applies pattern to relevant ontological context
**Output**: Structured prompt following proven templates

## Architecture Wins, Losses, and Trade-offs

### Major Wins
- **Superior metadata querying**: Firestore structured queries vs. ChromaDB metadata filtering
- **Remote backup capability**: Content safety against local machine failure
- **Multi-user potential**: No longer exclusively local-machine dependent
- **LLM comparison capability**: Local vs. external model performance analysis
- **Unified data visualization**: Firestore provides infrastructure for metadata visualization
- **Development velocity**: Firestore queries faster to develop/debug than ChromaDB filtering
- **Scalability headroom**: Architecture handles growth from hundreds to thousands of documents
- **128k token windows**: Local LLMs (Llama 3.1, Command R+, Qwen2) support substantial context

### Manageable Losses
- **Negligible external API costs**: Optional external LLM calls when desired
- **Complexity overhead**: 4-6 write operations per document increase failure points
- **Data consistency challenges**: Keeping multiple stores synchronized
- **Storage costs**: Content duplication across multiple systems

### Technical Implementation Plan

### Pass 1: Firestore-Based Recursive Filtering
**Status**: Major architectural revision - Firestore replaces ChromaDB for metadata queries
- **Metadata Queries**: Complex boolean filtering (`is_a="teaching-method" AND category="primary"`)
- **Multi-dimensional Filtering**: Tags, categories, ontological relationships in single queries
- **Recursive Operations**: Database queries with operation history stack
- **Semantic Enhancement**: Optional ChromaDB similarity search on filtered document sets
- **UI Components**: Metadata visualization + recursive operation controls

**Data Flow**: Firestore metadata → Document ID list → Optional ChromaDB semantic enhancement → Curated document set

### Metadata Visualization Tooling
**Status**: Firestore-enabled - much simpler implementation
- **Relationship Queries**: Native Firestore queries across ontological fields
- **On-Demand Generation**: Query and display metadata relationships in real-time
- **Expert Planning Interface**: Visual relationship mapping before recursive operations
- **Multi-dimensional Views**: Tags, categories, ontology relationships simultaneously

### Pass 2: Flexible LLM Processing
**Status**: Local + External options

**Pass 2A (Local Processing)**:
- **Models**: Llama 3.1, Command R+, Qwen2:72b-instruct (all 128k+ tokens)
- **Content Retrieval**: Firestore or ChromaDB depending on filtering path
- **Prompt Generation**: Multiple variations with substantial context capacity

**Pass 2B (External API Processing)**:
- **Models**: Claude, GPT, Gemini APIs
- **Use Cases**: Specialized prompt styles, performance comparisons
- **Cost Control**: Optional usage based on expert user requirements

### Expert User Interface Requirements
**Complexity Level**: Sophisticated multi-modal interface for expert users

**Pass 1 Interface**:
- **Metadata Visualization**: Firestore-powered relationship mapping across all metadata fields
- **Recursive Query Builder**: Complex boolean filtering with visual query construction
- **Operation History**: Undo/redo stack with descriptive operation breadcrumbs
- **Multi-dimensional Filtering**: Simultaneous ontological + tags + category + title filtering
- **Semantic Search Toggle**: Optional ChromaDB enhancement of filtered document sets

**Pass 2 Interface**:
- **LLM Model Selection**: Toggle between 3 local models (Llama 3.1, Command R+, Qwen2) and 3 external APIs (Claude, GPT, Gemini)
- **Context Management**: Display and edit curated document sets before prompt generation
- **Prompt Comparison**: Side-by-side results from different models
- **Template System**: Reusable prompt patterns and style instructions

**Data Management Interface**:
- **Sync Status**: Monitor consistency across 4-6 storage locations
- **Bulk Operations**: Efficient loading/updating with batch write operations
- **Error Recovery**: Handle partial write failures and inconsistencies

### Data Storage Integration Points

**Write Operations per Document (4-6 locations)**:
1. **Local Files**: ../RAG/[filename].md (source of truth)
2. **ChromaDB**: Vector embeddings + content (semantic search)
3. **Firestore Local**: Structured metadata (emulator for development)
4. **Firestore Local**: Full content backup (emulator for development)
5. **Firestore Remote**: Structured metadata (production backup)
6. **Firestore Remote**: Full content backup (production backup)

**Query Integration**:
- **Pass 1 Metadata**: Firestore structured queries
- **Pass 1 Semantic**: ChromaDB vector similarity (optional enhancement)
- **Pass 2 Content**: Firestore or ChromaDB content retrieval
- **Visualization**: Firestore relationship queries

**Consumer App Interface**: Data format preparation for downstream System 1 consumption

## Success Metrics

### Expert User Quality Metrics
- **Context Precision**: Can experts achieve more precise document selection than manual curation?
- **Iteration Efficiency**: How quickly can experts refine context through recursive operations?
- **Prompt Variation Quality**: Do generated prompt variations cover the intended range of styles/audiences?
- **External AI Performance**: Do our prompts produce better results in ChatGPT/Claude compared to manually crafted prompts?

### System Performance Metrics
- **Recursive Operation Speed**: How responsive are add/remove/undo operations on document sets?
- **Context Assembly Accuracy**: How well does ontological traversal discover relevant relationships?
- **Prompt Generation Consistency**: Do multiple generations from the same context maintain quality?

### Expert Workflow Metrics  
- **Tool Adoption**: Do expert prompt engineers prefer this over manual assembly?
- **Context Reusability**: How often do experts reuse refined context sets?
- **Consumer App Readiness**: How well do generated prompts serve downstream System 1 user needs?

## Implementation Roadmap

### Phase 1: Multi-Database Architecture Implementation (Next)
1. **Firestore Integration**: Replace ChromaDB metadata queries with Firestore structured queries
2. **Loader Enhancement**: Implement 4-6 write operations per document with error handling
3. **Metadata Visualization**: Firestore-powered relationship mapping and query planning
4. **Recursive Query Interface**: Complex boolean filtering with operation history
5. **Semantic Search Integration**: Optional ChromaDB enhancement of Firestore-filtered document sets
6. **Data Consistency Tools**: Sync verification and error recovery mechanisms

### Phase 2: Flexible LLM Pipeline Implementation
1. **Local LLM Integration**: Llama 3.1, Command R+, Qwen2 with 128k context windows
2. **External API Integration**: Claude, GPT, Gemini API calls (optional)
3. **Model Comparison Interface**: Side-by-side prompt generation results
4. **Template System**: Reusable prompt patterns and style variations
5. **Pass 1 → Pass 2 Handoff**: Document ID lists to content retrieval to prompt generation

### Phase 3: Expert User Optimization
1. **Advanced Ontological Operations**: Complex filtering patterns and saved queries
2. **Context Set Templates**: Reusable filtering patterns for common prompt types
3. **Performance Optimization**: Caching for recursive operations and prompt generation
4. **Consumer App Integration**: API design for prompt consumption by System 1 interfaces

### Validation Checkpoints
- **Feasibility Confirmed**: Recursive Pass 1 is standard software engineering complexity
- **User Base Clarified**: Expert-only tool (2-5 users initially)
- **Scope Boundaries**: Consumer app design explicitly out of scope
- **Technical Foundation**: Existing ontological filtering provides solid implementation base

---

*This document represents our current understanding and should evolve as we learn more about user needs and technical constraints.*